<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Linux 虚拟文件系统（虚拟内存文件系统）以及共享内存原理 | Memos</title>
<meta name=keywords content><meta name=description content="最近看了《Understanding the Linux® Virtual Memory Manager》里面的第十二章 SHARED MEMORY VIRTUAL FILESYSTEM ，对文件系统以及内存文件管理有了更加深入的了解，下面是看了这一章节之后对其中一些概念的理解以及拓展，要是想了解这一章，建议读原文，配合这篇博客辅助理解
Linux 哲学

在 Linux 里面，一切皆文件，所有的东西都可以看作一个文件，而凡是文件，都应该支持POSIX文件操作（比如 read() ，write() ，open()）
每一块内存对象，都可以被看作一个文件，一旦赋予这块内存对象相对应的文件描述，就可以使用像使用普通文件那样子操作内存对象
而这也正是 VFS（虚拟文件系统 virtual file system ，包括内存文件系统以及共享内存管理系统）的设计理念以及实现方向

VMA（virtual memory area）


Linux 内核用vm_area_struct结构体描述某一段连续的虚拟内存区域VMA（virtual memory area），每个虚拟内存区域 VMA 都有自己的vm_area_struct 结构体


内存描述符 mm_struct 指向进程的整个地址空间，vm_area_struct 只是指向了虚拟空间的一段，这块虚拟内存区域VMA的地址范围为 [vm_start, vm_end) ，左开右闭



vm_area_struct 是由双向链表链接起来的，它们是按照虚拟地址降序排序的，每个这样的结构都对应描述一个地址空间范围


为了快速根据地址找到对应的 VMA，内核对其建立了红黑树索引，红黑树的每个叶子结点就是一个VMA区域，引入红黑树的好处是可以提高查找VMA的效率（即便VMA的数量翻倍，VMA的查找次数也只增加一次）

    
    
    
        
    




之所以这样分隔是因为每个虚拟区间可能来源不同，有的可能来自可执行映像，有的可能来自共享库，而有的可能是动态内存分配的内存区，所以对于每个由 vm_area_struct 结构所描述的区间的处理操作和它前后范围的处理操作不同，因此 linux 把虚拟内存分割管理，并利用了虚拟内存处理例程 vm_ops 来抽象对不同来源虚拟内存的处理方法


不同的虚拟区间其处理操作可能不同，linux 在这里利用了面向对象的思想，即把一个虚拟区间看成是一个对象，用 vm_area_struct 描述这个对象的属性，其中的 vm_operation 结构描述了在这个对象上的操作

    
    
    
        
    




虚拟内存空间管理概括图

    
    
    
        
    




 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31


# https://elixir.bootlin.com/linux/v2.6.0/source/include/linux/mm.h#L51
struct vm_area_struct {
	struct mm_struct * vm_mm;	/* The address space we belong to. */
	unsigned long vm_start;		/* Our start address within vm_mm. */
	unsigned long vm_end;		/* The first byte after our end address
					   within vm_mm. */

	/* linked list of VM areas per task, sorted by address */
	struct vm_area_struct *vm_next;

	pgprot_t vm_page_prot;		/* Access permissions of this VMA. */
	unsigned long vm_flags;		/* Flags, listed below. */

	struct rb_node vm_rb;

	/*
	 * For areas with an address space and backing store,
	 * one of the address_space->i_mmap{,shared} lists,
	 * for shm areas, the list of attaches, otherwise unused.
	 */
	struct list_head shared;

	/* Function pointers to deal with this struct. */
	struct vm_operations_struct * vm_ops;

	/* Information about our backing store: */
	unsigned long vm_pgoff;		/* Offset (within vm_file) in PAGE_SIZE
					   units, *not* PAGE_CACHE_SIZE */
	struct file * vm_file;		/* File we map to (can be NULL). */
	void * vm_private_data;		/* was vm_pte (shared mem) */
};




struct address_space

看 linux 内核很容易被 struct address_space 这个结构迷惑，它是代表某个地址空间吗？实际上不是的，它是用于管理文件 struct inode 映射到内存的页面 struct page 的，其实就是每个 file 都有这么一个结构，将文件系统中这个 file 对应的数据与这个 file 对应的内存绑定到一起
与之对应，address_space_operations 就是用来操作该文件映射到内存的页面，比如把内存中的修改写回文件、从文件中读入数据到页面缓冲等
一个具体的文件在打开后，内核会在内存中为之建立一个 struct inode 结构（该 inode 结构也会在对应的 file 结构体中引用），其中的 i_mapping 域指向一个 address_space 结构
一个文件就对应一个 address_space 结构，一个 address_space 与一个偏移量能够确定一个 page cache 或 swap cache 中的一个页面，当要寻址某个数据时，很容易根据给定的文件及数据在文件内的偏移量而找到相应的页面
struct file 和 struct inode 结构体中都有一个 struct address_space 指针，实际上，file -> f_mapping 是从对应 inode -> i_mapping 而来, inode -> i_mapping -> a_ops 是由对应的文件系统类型在生成这个 inode 时赋予的


 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22


# https://elixir.bootlin.com/linux/v2.6.0/source/include/linux/fs.h#L319
struct address_space {
	struct inode		*host;		/* owner: inode, block_device */
	struct radix_tree_root	page_tree;	/* radix tree of all pages */
	spinlock_t		page_lock;	/* and spinlock protecting it */
	struct list_head	clean_pages;	/* list of clean pages */
	struct list_head	dirty_pages;	/* list of dirty pages */
	struct list_head	locked_pages;	/* list of locked pages */
	struct list_head	io_pages;	/* being prepared for I/O */
	unsigned long		nrpages;	/* number of total pages */
	struct address_space_operations *a_ops;	/* methods */
	struct list_head	i_mmap;		/* list of private mappings */
	struct list_head	i_mmap_shared;	/* list of shared mappings */
	struct semaphore	i_shared_sem;	/* protect both above lists */
	atomic_t		truncate_count;	/* Cover race condition with truncate */
	unsigned long		dirtied_when;	/* jiffies of first page dirtying */
	unsigned long		flags;		/* error bits/gfp mask */
	struct backing_dev_info *backing_dev_info; /* device readahead, etc */
	spinlock_t		private_lock;	/* for use by the address_space */
	struct list_head	private_list;	/* ditto */
	struct address_space	*assoc_mapping;	/* ditto */
};




文件系统、文件类型、page 的划分

为了方便理解，在此将文件系统划分为内存文件系统（虚拟文件系统）与硬盘文件系统（物理文件系统），在书的这一章节里面，也将广义上的文件分为 virtual file 和 physics file
在书的这一章节里面，将内存页面 page 划分为 anonymous pages （没有物理文件支持的内存页面）与 pages backed by a file（由物理文件映射到内存的某些 pages）

page cache 与 swap cache

page cache 是与文件映射对应的，而 swap cache 是与匿名页对应的
如果一个内存页面不是文件映射，则在换入换出的时候加入到 swap cache ，如果是文件映射，则不需要交换缓冲
这两个的相同点就是它们都是 address_space ，都有相对应的文件操作：一个被访问的文件的物理页面都驻留在 page cache 或 swap cache 中，一个页面的所有信息由 struct page 来描述
page cache 作用：当文件被读取时，操作系统会把文件内容加载到内存的 page cache 中。如果同一个文件被再次访问，操作系统会直接从 page cache 中读取，而不是再次从磁盘读取，从而减少磁盘访问次数，提高性能
swap cache 作用：swap cache 缓存的是已经交换到磁盘的数据，它是为了提高交换操作的效率，优化虚拟内存的交换操作，如果内存中再次需要使用这些页面，操作系统会从 swap cache 里面寻找，然后再从 swap 里面查找，同时对于那些刚刚从物理内存里面换出来的 page 以及从 swap 空间读取的 page 也会放在 swap cache 里面
一般情况下用户进程调用 mmap() 时，只是在进程空间内新增了一块相应大小的缓冲区，并设置了相应的访问标识，但并没有建立进程空间到物理页面的映射
因此，第一次访问该空间时，会引发一个缺页异常。对于共享内存映射情况，缺页异常处理程序首先在 swap cache 中寻找目标页（符合 address_space 以及偏移量的物理页），如果找到，则直接返回地址；如果没有找到，则判断该页是否在交换区 (swap area)，如果在，则执行一个换入操作；如果上述两种情况都不满足，处理程序将分配新的物理页面，并把它插入到 page cache 中，程最终将更新进程页表
对于映射普通文件情况（非共享映射），缺页异常处理程序首先会在 page cache 中根据 address_space 以及数据偏移量寻找相应的页面，如果没有找到，则说明文件数据还没有读入内存，处理程序会从磁盘读入相应的页面，并返回相应地址，同时，进程页表也会更新

硬盘（物理）文件系统
tmpfs 和 shm 共性与区别
tmpfs 和 shm 都是基于内存的文件系统，它们将文件存储在内存中，而不是磁盘上，尽管它们有一些相似之处，但它们的用途、配置方法以及操作上也有一些关键的区别"><meta name=author content="HCY"><link rel=canonical href=https://hcy-asleep.github.io/Linux-%E8%99%9A%E6%8B%9F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%BB%A5%E5%8F%8A%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98%E5%8E%9F%E7%90%86/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.bf546705000388ff8f0176e72d11be7c3d4efd591a0430f9c62915164a160ae2.css integrity="sha256-v1RnBQADiP+PAXbnLRG+fD1O/VkaBDD5xikVFkoWCuI=" rel="preload stylesheet" as=style><link rel=icon href=https://raw.githubusercontent.com/HCY-ASLEEP/picture-bed/main/picture-bed/hcy_site_favicon.png><link rel=icon type=image/png sizes=16x16 href=https://raw.githubusercontent.com/HCY-ASLEEP/picture-bed/main/picture-bed/hcy_site_favicon.png><link rel=icon type=image/png sizes=32x32 href=https://raw.githubusercontent.com/HCY-ASLEEP/picture-bed/main/picture-bed/hcy_site_favicon.png><link rel=apple-touch-icon href=https://raw.githubusercontent.com/HCY-ASLEEP/picture-bed/main/picture-bed/hcy_site_favicon.png><link rel=mask-icon href=https://raw.githubusercontent.com/HCY-ASLEEP/picture-bed/main/picture-bed/hcy_site_favicon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://hcy-asleep.github.io/Linux-%E8%99%9A%E6%8B%9F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%BB%A5%E5%8F%8A%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98%E5%8E%9F%E7%90%86/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><style>@media screen and (min-width:1px){.post-content input[type=checkbox]:checked~label>img{transform:scale(1.6);cursor:zoom-out;position:relative;z-index:999}.post-content img.zoomCheck{transition:transform .15s ease;z-index:999;cursor:zoom-in}}</style><meta property="og:title" content="Linux 虚拟文件系统（虚拟内存文件系统）以及共享内存原理"><meta property="og:description" content="最近看了《Understanding the Linux® Virtual Memory Manager》里面的第十二章 SHARED MEMORY VIRTUAL FILESYSTEM ，对文件系统以及内存文件管理有了更加深入的了解，下面是看了这一章节之后对其中一些概念的理解以及拓展，要是想了解这一章，建议读原文，配合这篇博客辅助理解
Linux 哲学

在 Linux 里面，一切皆文件，所有的东西都可以看作一个文件，而凡是文件，都应该支持POSIX文件操作（比如 read() ，write() ，open()）
每一块内存对象，都可以被看作一个文件，一旦赋予这块内存对象相对应的文件描述，就可以使用像使用普通文件那样子操作内存对象
而这也正是 VFS（虚拟文件系统 virtual file system ，包括内存文件系统以及共享内存管理系统）的设计理念以及实现方向

VMA（virtual memory area）


Linux 内核用vm_area_struct结构体描述某一段连续的虚拟内存区域VMA（virtual memory area），每个虚拟内存区域 VMA 都有自己的vm_area_struct 结构体


内存描述符 mm_struct 指向进程的整个地址空间，vm_area_struct 只是指向了虚拟空间的一段，这块虚拟内存区域VMA的地址范围为 [vm_start, vm_end) ，左开右闭



vm_area_struct 是由双向链表链接起来的，它们是按照虚拟地址降序排序的，每个这样的结构都对应描述一个地址空间范围


为了快速根据地址找到对应的 VMA，内核对其建立了红黑树索引，红黑树的每个叶子结点就是一个VMA区域，引入红黑树的好处是可以提高查找VMA的效率（即便VMA的数量翻倍，VMA的查找次数也只增加一次）

    
    
    
        
    




之所以这样分隔是因为每个虚拟区间可能来源不同，有的可能来自可执行映像，有的可能来自共享库，而有的可能是动态内存分配的内存区，所以对于每个由 vm_area_struct 结构所描述的区间的处理操作和它前后范围的处理操作不同，因此 linux 把虚拟内存分割管理，并利用了虚拟内存处理例程 vm_ops 来抽象对不同来源虚拟内存的处理方法


不同的虚拟区间其处理操作可能不同，linux 在这里利用了面向对象的思想，即把一个虚拟区间看成是一个对象，用 vm_area_struct 描述这个对象的属性，其中的 vm_operation 结构描述了在这个对象上的操作

    
    
    
        
    




虚拟内存空间管理概括图

    
    
    
        
    




 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31


# https://elixir.bootlin.com/linux/v2.6.0/source/include/linux/mm.h#L51
struct vm_area_struct {
	struct mm_struct * vm_mm;	/* The address space we belong to. */
	unsigned long vm_start;		/* Our start address within vm_mm. */
	unsigned long vm_end;		/* The first byte after our end address
					   within vm_mm. */

	/* linked list of VM areas per task, sorted by address */
	struct vm_area_struct *vm_next;

	pgprot_t vm_page_prot;		/* Access permissions of this VMA. */
	unsigned long vm_flags;		/* Flags, listed below. */

	struct rb_node vm_rb;

	/*
	 * For areas with an address space and backing store,
	 * one of the address_space->i_mmap{,shared} lists,
	 * for shm areas, the list of attaches, otherwise unused.
	 */
	struct list_head shared;

	/* Function pointers to deal with this struct. */
	struct vm_operations_struct * vm_ops;

	/* Information about our backing store: */
	unsigned long vm_pgoff;		/* Offset (within vm_file) in PAGE_SIZE
					   units, *not* PAGE_CACHE_SIZE */
	struct file * vm_file;		/* File we map to (can be NULL). */
	void * vm_private_data;		/* was vm_pte (shared mem) */
};




struct address_space

看 linux 内核很容易被 struct address_space 这个结构迷惑，它是代表某个地址空间吗？实际上不是的，它是用于管理文件 struct inode 映射到内存的页面 struct page 的，其实就是每个 file 都有这么一个结构，将文件系统中这个 file 对应的数据与这个 file 对应的内存绑定到一起
与之对应，address_space_operations 就是用来操作该文件映射到内存的页面，比如把内存中的修改写回文件、从文件中读入数据到页面缓冲等
一个具体的文件在打开后，内核会在内存中为之建立一个 struct inode 结构（该 inode 结构也会在对应的 file 结构体中引用），其中的 i_mapping 域指向一个 address_space 结构
一个文件就对应一个 address_space 结构，一个 address_space 与一个偏移量能够确定一个 page cache 或 swap cache 中的一个页面，当要寻址某个数据时，很容易根据给定的文件及数据在文件内的偏移量而找到相应的页面
struct file 和 struct inode 结构体中都有一个 struct address_space 指针，实际上，file -> f_mapping 是从对应 inode -> i_mapping 而来, inode -> i_mapping -> a_ops 是由对应的文件系统类型在生成这个 inode 时赋予的


 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22


# https://elixir.bootlin.com/linux/v2.6.0/source/include/linux/fs.h#L319
struct address_space {
	struct inode		*host;		/* owner: inode, block_device */
	struct radix_tree_root	page_tree;	/* radix tree of all pages */
	spinlock_t		page_lock;	/* and spinlock protecting it */
	struct list_head	clean_pages;	/* list of clean pages */
	struct list_head	dirty_pages;	/* list of dirty pages */
	struct list_head	locked_pages;	/* list of locked pages */
	struct list_head	io_pages;	/* being prepared for I/O */
	unsigned long		nrpages;	/* number of total pages */
	struct address_space_operations *a_ops;	/* methods */
	struct list_head	i_mmap;		/* list of private mappings */
	struct list_head	i_mmap_shared;	/* list of shared mappings */
	struct semaphore	i_shared_sem;	/* protect both above lists */
	atomic_t		truncate_count;	/* Cover race condition with truncate */
	unsigned long		dirtied_when;	/* jiffies of first page dirtying */
	unsigned long		flags;		/* error bits/gfp mask */
	struct backing_dev_info *backing_dev_info; /* device readahead, etc */
	spinlock_t		private_lock;	/* for use by the address_space */
	struct list_head	private_list;	/* ditto */
	struct address_space	*assoc_mapping;	/* ditto */
};




文件系统、文件类型、page 的划分

为了方便理解，在此将文件系统划分为内存文件系统（虚拟文件系统）与硬盘文件系统（物理文件系统），在书的这一章节里面，也将广义上的文件分为 virtual file 和 physics file
在书的这一章节里面，将内存页面 page 划分为 anonymous pages （没有物理文件支持的内存页面）与 pages backed by a file（由物理文件映射到内存的某些 pages）

page cache 与 swap cache

page cache 是与文件映射对应的，而 swap cache 是与匿名页对应的
如果一个内存页面不是文件映射，则在换入换出的时候加入到 swap cache ，如果是文件映射，则不需要交换缓冲
这两个的相同点就是它们都是 address_space ，都有相对应的文件操作：一个被访问的文件的物理页面都驻留在 page cache 或 swap cache 中，一个页面的所有信息由 struct page 来描述
page cache 作用：当文件被读取时，操作系统会把文件内容加载到内存的 page cache 中。如果同一个文件被再次访问，操作系统会直接从 page cache 中读取，而不是再次从磁盘读取，从而减少磁盘访问次数，提高性能
swap cache 作用：swap cache 缓存的是已经交换到磁盘的数据，它是为了提高交换操作的效率，优化虚拟内存的交换操作，如果内存中再次需要使用这些页面，操作系统会从 swap cache 里面寻找，然后再从 swap 里面查找，同时对于那些刚刚从物理内存里面换出来的 page 以及从 swap 空间读取的 page 也会放在 swap cache 里面
一般情况下用户进程调用 mmap() 时，只是在进程空间内新增了一块相应大小的缓冲区，并设置了相应的访问标识，但并没有建立进程空间到物理页面的映射
因此，第一次访问该空间时，会引发一个缺页异常。对于共享内存映射情况，缺页异常处理程序首先在 swap cache 中寻找目标页（符合 address_space 以及偏移量的物理页），如果找到，则直接返回地址；如果没有找到，则判断该页是否在交换区 (swap area)，如果在，则执行一个换入操作；如果上述两种情况都不满足，处理程序将分配新的物理页面，并把它插入到 page cache 中，程最终将更新进程页表
对于映射普通文件情况（非共享映射），缺页异常处理程序首先会在 page cache 中根据 address_space 以及数据偏移量寻找相应的页面，如果没有找到，则说明文件数据还没有读入内存，处理程序会从磁盘读入相应的页面，并返回相应地址，同时，进程页表也会更新

硬盘（物理）文件系统
tmpfs 和 shm 共性与区别
tmpfs 和 shm 都是基于内存的文件系统，它们将文件存储在内存中，而不是磁盘上，尽管它们有一些相似之处，但它们的用途、配置方法以及操作上也有一些关键的区别"><meta property="og:type" content="article"><meta property="og:url" content="https://hcy-asleep.github.io/Linux-%E8%99%9A%E6%8B%9F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%BB%A5%E5%8F%8A%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98%E5%8E%9F%E7%90%86/"><meta property="og:image" content="https://hcy-asleep.github.io/"><meta property="article:section" content="post"><meta property="article:published_time" content="2024-11-30T19:35:39+00:00"><meta property="article:modified_time" content="2024-11-30T19:35:39+00:00"><meta property="og:site_name" content="HCY-BLOGS"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://hcy-asleep.github.io/"><meta name=twitter:title content="Linux 虚拟文件系统（虚拟内存文件系统）以及共享内存原理"><meta name=twitter:description content="最近看了《Understanding the Linux® Virtual Memory Manager》里面的第十二章 SHARED MEMORY VIRTUAL FILESYSTEM ，对文件系统以及内存文件管理有了更加深入的了解，下面是看了这一章节之后对其中一些概念的理解以及拓展，要是想了解这一章，建议读原文，配合这篇博客辅助理解
Linux 哲学

在 Linux 里面，一切皆文件，所有的东西都可以看作一个文件，而凡是文件，都应该支持POSIX文件操作（比如 read() ，write() ，open()）
每一块内存对象，都可以被看作一个文件，一旦赋予这块内存对象相对应的文件描述，就可以使用像使用普通文件那样子操作内存对象
而这也正是 VFS（虚拟文件系统 virtual file system ，包括内存文件系统以及共享内存管理系统）的设计理念以及实现方向

VMA（virtual memory area）


Linux 内核用vm_area_struct结构体描述某一段连续的虚拟内存区域VMA（virtual memory area），每个虚拟内存区域 VMA 都有自己的vm_area_struct 结构体


内存描述符 mm_struct 指向进程的整个地址空间，vm_area_struct 只是指向了虚拟空间的一段，这块虚拟内存区域VMA的地址范围为 [vm_start, vm_end) ，左开右闭



vm_area_struct 是由双向链表链接起来的，它们是按照虚拟地址降序排序的，每个这样的结构都对应描述一个地址空间范围


为了快速根据地址找到对应的 VMA，内核对其建立了红黑树索引，红黑树的每个叶子结点就是一个VMA区域，引入红黑树的好处是可以提高查找VMA的效率（即便VMA的数量翻倍，VMA的查找次数也只增加一次）

    
    
    
        
    




之所以这样分隔是因为每个虚拟区间可能来源不同，有的可能来自可执行映像，有的可能来自共享库，而有的可能是动态内存分配的内存区，所以对于每个由 vm_area_struct 结构所描述的区间的处理操作和它前后范围的处理操作不同，因此 linux 把虚拟内存分割管理，并利用了虚拟内存处理例程 vm_ops 来抽象对不同来源虚拟内存的处理方法


不同的虚拟区间其处理操作可能不同，linux 在这里利用了面向对象的思想，即把一个虚拟区间看成是一个对象，用 vm_area_struct 描述这个对象的属性，其中的 vm_operation 结构描述了在这个对象上的操作

    
    
    
        
    




虚拟内存空间管理概括图

    
    
    
        
    




 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31


# https://elixir.bootlin.com/linux/v2.6.0/source/include/linux/mm.h#L51
struct vm_area_struct {
	struct mm_struct * vm_mm;	/* The address space we belong to. */
	unsigned long vm_start;		/* Our start address within vm_mm. */
	unsigned long vm_end;		/* The first byte after our end address
					   within vm_mm. */

	/* linked list of VM areas per task, sorted by address */
	struct vm_area_struct *vm_next;

	pgprot_t vm_page_prot;		/* Access permissions of this VMA. */
	unsigned long vm_flags;		/* Flags, listed below. */

	struct rb_node vm_rb;

	/*
	 * For areas with an address space and backing store,
	 * one of the address_space->i_mmap{,shared} lists,
	 * for shm areas, the list of attaches, otherwise unused.
	 */
	struct list_head shared;

	/* Function pointers to deal with this struct. */
	struct vm_operations_struct * vm_ops;

	/* Information about our backing store: */
	unsigned long vm_pgoff;		/* Offset (within vm_file) in PAGE_SIZE
					   units, *not* PAGE_CACHE_SIZE */
	struct file * vm_file;		/* File we map to (can be NULL). */
	void * vm_private_data;		/* was vm_pte (shared mem) */
};




struct address_space

看 linux 内核很容易被 struct address_space 这个结构迷惑，它是代表某个地址空间吗？实际上不是的，它是用于管理文件 struct inode 映射到内存的页面 struct page 的，其实就是每个 file 都有这么一个结构，将文件系统中这个 file 对应的数据与这个 file 对应的内存绑定到一起
与之对应，address_space_operations 就是用来操作该文件映射到内存的页面，比如把内存中的修改写回文件、从文件中读入数据到页面缓冲等
一个具体的文件在打开后，内核会在内存中为之建立一个 struct inode 结构（该 inode 结构也会在对应的 file 结构体中引用），其中的 i_mapping 域指向一个 address_space 结构
一个文件就对应一个 address_space 结构，一个 address_space 与一个偏移量能够确定一个 page cache 或 swap cache 中的一个页面，当要寻址某个数据时，很容易根据给定的文件及数据在文件内的偏移量而找到相应的页面
struct file 和 struct inode 结构体中都有一个 struct address_space 指针，实际上，file -> f_mapping 是从对应 inode -> i_mapping 而来, inode -> i_mapping -> a_ops 是由对应的文件系统类型在生成这个 inode 时赋予的


 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22


# https://elixir.bootlin.com/linux/v2.6.0/source/include/linux/fs.h#L319
struct address_space {
	struct inode		*host;		/* owner: inode, block_device */
	struct radix_tree_root	page_tree;	/* radix tree of all pages */
	spinlock_t		page_lock;	/* and spinlock protecting it */
	struct list_head	clean_pages;	/* list of clean pages */
	struct list_head	dirty_pages;	/* list of dirty pages */
	struct list_head	locked_pages;	/* list of locked pages */
	struct list_head	io_pages;	/* being prepared for I/O */
	unsigned long		nrpages;	/* number of total pages */
	struct address_space_operations *a_ops;	/* methods */
	struct list_head	i_mmap;		/* list of private mappings */
	struct list_head	i_mmap_shared;	/* list of shared mappings */
	struct semaphore	i_shared_sem;	/* protect both above lists */
	atomic_t		truncate_count;	/* Cover race condition with truncate */
	unsigned long		dirtied_when;	/* jiffies of first page dirtying */
	unsigned long		flags;		/* error bits/gfp mask */
	struct backing_dev_info *backing_dev_info; /* device readahead, etc */
	spinlock_t		private_lock;	/* for use by the address_space */
	struct list_head	private_list;	/* ditto */
	struct address_space	*assoc_mapping;	/* ditto */
};




文件系统、文件类型、page 的划分

为了方便理解，在此将文件系统划分为内存文件系统（虚拟文件系统）与硬盘文件系统（物理文件系统），在书的这一章节里面，也将广义上的文件分为 virtual file 和 physics file
在书的这一章节里面，将内存页面 page 划分为 anonymous pages （没有物理文件支持的内存页面）与 pages backed by a file（由物理文件映射到内存的某些 pages）

page cache 与 swap cache

page cache 是与文件映射对应的，而 swap cache 是与匿名页对应的
如果一个内存页面不是文件映射，则在换入换出的时候加入到 swap cache ，如果是文件映射，则不需要交换缓冲
这两个的相同点就是它们都是 address_space ，都有相对应的文件操作：一个被访问的文件的物理页面都驻留在 page cache 或 swap cache 中，一个页面的所有信息由 struct page 来描述
page cache 作用：当文件被读取时，操作系统会把文件内容加载到内存的 page cache 中。如果同一个文件被再次访问，操作系统会直接从 page cache 中读取，而不是再次从磁盘读取，从而减少磁盘访问次数，提高性能
swap cache 作用：swap cache 缓存的是已经交换到磁盘的数据，它是为了提高交换操作的效率，优化虚拟内存的交换操作，如果内存中再次需要使用这些页面，操作系统会从 swap cache 里面寻找，然后再从 swap 里面查找，同时对于那些刚刚从物理内存里面换出来的 page 以及从 swap 空间读取的 page 也会放在 swap cache 里面
一般情况下用户进程调用 mmap() 时，只是在进程空间内新增了一块相应大小的缓冲区，并设置了相应的访问标识，但并没有建立进程空间到物理页面的映射
因此，第一次访问该空间时，会引发一个缺页异常。对于共享内存映射情况，缺页异常处理程序首先在 swap cache 中寻找目标页（符合 address_space 以及偏移量的物理页），如果找到，则直接返回地址；如果没有找到，则判断该页是否在交换区 (swap area)，如果在，则执行一个换入操作；如果上述两种情况都不满足，处理程序将分配新的物理页面，并把它插入到 page cache 中，程最终将更新进程页表
对于映射普通文件情况（非共享映射），缺页异常处理程序首先会在 page cache 中根据 address_space 以及数据偏移量寻找相应的页面，如果没有找到，则说明文件数据还没有读入内存，处理程序会从磁盘读入相应的页面，并返回相应地址，同时，进程页表也会更新

硬盘（物理）文件系统
tmpfs 和 shm 共性与区别
tmpfs 和 shm 都是基于内存的文件系统，它们将文件存储在内存中，而不是磁盘上，尽管它们有一些相似之处，但它们的用途、配置方法以及操作上也有一些关键的区别"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://hcy-asleep.github.io/post/"},{"@type":"ListItem","position":2,"name":"Linux 虚拟文件系统（虚拟内存文件系统）以及共享内存原理","item":"https://hcy-asleep.github.io/Linux-%E8%99%9A%E6%8B%9F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%BB%A5%E5%8F%8A%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98%E5%8E%9F%E7%90%86/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Linux 虚拟文件系统（虚拟内存文件系统）以及共享内存原理","name":"Linux 虚拟文件系统（虚拟内存文件系统）以及共享内存原理","description":"最近看了《Understanding the Linux® Virtual Memory Manager》里面的第十二章 SHARED MEMORY VIRTUAL FILESYSTEM ，对文件系统以及内存文件管理有了更加深入的了解，下面是看了这一章节之后对其中一些概念的理解以及拓展，要是想了解这一章，建议读原文，配合这篇博客辅助理解\nLinux 哲学 在 Linux 里面，一切皆文件，所有的东西都可以看作一个文件，而凡是文件，都应该支持POSIX文件操作（比如 read() ，write() ，open()） 每一块内存对象，都可以被看作一个文件，一旦赋予这块内存对象相对应的文件描述，就可以使用像使用普通文件那样子操作内存对象 而这也正是 VFS（虚拟文件系统 virtual file system ，包括内存文件系统以及共享内存管理系统）的设计理念以及实现方向 VMA（virtual memory area） Linux 内核用vm_area_struct结构体描述某一段连续的虚拟内存区域VMA（virtual memory area），每个虚拟内存区域 VMA 都有自己的vm_area_struct 结构体\n内存描述符 mm_struct 指向进程的整个地址空间，vm_area_struct 只是指向了虚拟空间的一段，这块虚拟内存区域VMA的地址范围为 [vm_start, vm_end) ，左开右闭 vm_area_struct 是由双向链表链接起来的，它们是按照虚拟地址降序排序的，每个这样的结构都对应描述一个地址空间范围\n为了快速根据地址找到对应的 VMA，内核对其建立了红黑树索引，红黑树的每个叶子结点就是一个VMA区域，引入红黑树的好处是可以提高查找VMA的效率（即便VMA的数量翻倍，VMA的查找次数也只增加一次） 之所以这样分隔是因为每个虚拟区间可能来源不同，有的可能来自可执行映像，有的可能来自共享库，而有的可能是动态内存分配的内存区，所以对于每个由 vm_area_struct 结构所描述的区间的处理操作和它前后范围的处理操作不同，因此 linux 把虚拟内存分割管理，并利用了虚拟内存处理例程 vm_ops 来抽象对不同来源虚拟内存的处理方法\n不同的虚拟区间其处理操作可能不同，linux 在这里利用了面向对象的思想，即把一个虚拟区间看成是一个对象，用 vm_area_struct 描述这个对象的属性，其中的 vm_operation 结构描述了在这个对象上的操作 虚拟内存空间管理概括图 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # https://elixir.bootlin.com/linux/v2.6.0/source/include/linux/mm.h#L51 struct vm_area_struct { struct mm_struct * vm_mm;\t/* The address space we belong to. */ unsigned long vm_start;\t/* Our start address within vm_mm. */ unsigned long vm_end;\t/* The first byte after our end address within vm_mm. */ /* linked list of VM areas per task, sorted by address */ struct vm_area_struct *vm_next; pgprot_t vm_page_prot;\t/* Access permissions of this VMA. */ unsigned long vm_flags;\t/* Flags, listed below. */ struct rb_node vm_rb; /* * For areas with an address space and backing store, * one of the address_space-\u0026gt;i_mmap{,shared} lists, * for shm areas, the list of attaches, otherwise unused. */ struct list_head shared; /* Function pointers to deal with this struct. */ struct vm_operations_struct * vm_ops; /* Information about our backing store: */ unsigned long vm_pgoff;\t/* Offset (within vm_file) in PAGE_SIZE units, *not* PAGE_CACHE_SIZE */ struct file * vm_file;\t/* File we map to (can be NULL). */ void * vm_private_data;\t/* was vm_pte (shared mem) */ }; struct address_space 看 linux 内核很容易被 struct address_space 这个结构迷惑，它是代表某个地址空间吗？实际上不是的，它是用于管理文件 struct inode 映射到内存的页面 struct page 的，其实就是每个 file 都有这么一个结构，将文件系统中这个 file 对应的数据与这个 file 对应的内存绑定到一起 与之对应，address_space_operations 就是用来操作该文件映射到内存的页面，比如把内存中的修改写回文件、从文件中读入数据到页面缓冲等 一个具体的文件在打开后，内核会在内存中为之建立一个 struct inode 结构（该 inode 结构也会在对应的 file 结构体中引用），其中的 i_mapping 域指向一个 address_space 结构 一个文件就对应一个 address_space 结构，一个 address_space 与一个偏移量能够确定一个 page cache 或 swap cache 中的一个页面，当要寻址某个数据时，很容易根据给定的文件及数据在文件内的偏移量而找到相应的页面 struct file 和 struct inode 结构体中都有一个 struct address_space 指针，实际上，file -\u0026gt; f_mapping 是从对应 inode -\u0026gt; i_mapping 而来, inode -\u0026gt; i_mapping -\u0026gt; a_ops 是由对应的文件系统类型在生成这个 inode 时赋予的 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # https://elixir.bootlin.com/linux/v2.6.0/source/include/linux/fs.h#L319 struct address_space { struct inode\t*host;\t/* owner: inode, block_device */ struct radix_tree_root\tpage_tree;\t/* radix tree of all pages */ spinlock_t\tpage_lock;\t/* and spinlock protecting it */ struct list_head\tclean_pages;\t/* list of clean pages */ struct list_head\tdirty_pages;\t/* list of dirty pages */ struct list_head\tlocked_pages;\t/* list of locked pages */ struct list_head\tio_pages;\t/* being prepared for I/O */ unsigned long\tnrpages;\t/* number of total pages */ struct address_space_operations *a_ops;\t/* methods */ struct list_head\ti_mmap;\t/* list of private mappings */ struct list_head\ti_mmap_shared;\t/* list of shared mappings */ struct semaphore\ti_shared_sem;\t/* protect both above lists */ atomic_t\ttruncate_count;\t/* Cover race condition with truncate */ unsigned long\tdirtied_when;\t/* jiffies of first page dirtying */ unsigned long\tflags;\t/* error bits/gfp mask */ struct backing_dev_info *backing_dev_info; /* device readahead, etc */ spinlock_t\tprivate_lock;\t/* for use by the address_space */ struct list_head\tprivate_list;\t/* ditto */ struct address_space\t*assoc_mapping;\t/* ditto */ }; 文件系统、文件类型、page 的划分 为了方便理解，在此将文件系统划分为内存文件系统（虚拟文件系统）与硬盘文件系统（物理文件系统），在书的这一章节里面，也将广义上的文件分为 virtual file 和 physics file 在书的这一章节里面，将内存页面 page 划分为 anonymous pages （没有物理文件支持的内存页面）与 pages backed by a file（由物理文件映射到内存的某些 pages） page cache 与 swap cache page cache 是与文件映射对应的，而 swap cache 是与匿名页对应的 如果一个内存页面不是文件映射，则在换入换出的时候加入到 swap cache ，如果是文件映射，则不需要交换缓冲 这两个的相同点就是它们都是 address_space ，都有相对应的文件操作：一个被访问的文件的物理页面都驻留在 page cache 或 swap cache 中，一个页面的所有信息由 struct page 来描述 page cache 作用：当文件被读取时，操作系统会把文件内容加载到内存的 page cache 中。如果同一个文件被再次访问，操作系统会直接从 page cache 中读取，而不是再次从磁盘读取，从而减少磁盘访问次数，提高性能 swap cache 作用：swap cache 缓存的是已经交换到磁盘的数据，它是为了提高交换操作的效率，优化虚拟内存的交换操作，如果内存中再次需要使用这些页面，操作系统会从 swap cache 里面寻找，然后再从 swap 里面查找，同时对于那些刚刚从物理内存里面换出来的 page 以及从 swap 空间读取的 page 也会放在 swap cache 里面 一般情况下用户进程调用 mmap() 时，只是在进程空间内新增了一块相应大小的缓冲区，并设置了相应的访问标识，但并没有建立进程空间到物理页面的映射 因此，第一次访问该空间时，会引发一个缺页异常。对于共享内存映射情况，缺页异常处理程序首先在 swap cache 中寻找目标页（符合 address_space 以及偏移量的物理页），如果找到，则直接返回地址；如果没有找到，则判断该页是否在交换区 (swap area)，如果在，则执行一个换入操作；如果上述两种情况都不满足，处理程序将分配新的物理页面，并把它插入到 page cache 中，程最终将更新进程页表 对于映射普通文件情况（非共享映射），缺页异常处理程序首先会在 page cache 中根据 address_space 以及数据偏移量寻找相应的页面，如果没有找到，则说明文件数据还没有读入内存，处理程序会从磁盘读入相应的页面，并返回相应地址，同时，进程页表也会更新 硬盘（物理）文件系统 tmpfs 和 shm 共性与区别 tmpfs 和 shm 都是基于内存的文件系统，它们将文件存储在内存中，而不是磁盘上，尽管它们有一些相似之处，但它们的用途、配置方法以及操作上也有一些关键的区别\n","keywords":[],"articleBody":"最近看了《Understanding the Linux® Virtual Memory Manager》里面的第十二章 SHARED MEMORY VIRTUAL FILESYSTEM ，对文件系统以及内存文件管理有了更加深入的了解，下面是看了这一章节之后对其中一些概念的理解以及拓展，要是想了解这一章，建议读原文，配合这篇博客辅助理解\nLinux 哲学 在 Linux 里面，一切皆文件，所有的东西都可以看作一个文件，而凡是文件，都应该支持POSIX文件操作（比如 read() ，write() ，open()） 每一块内存对象，都可以被看作一个文件，一旦赋予这块内存对象相对应的文件描述，就可以使用像使用普通文件那样子操作内存对象 而这也正是 VFS（虚拟文件系统 virtual file system ，包括内存文件系统以及共享内存管理系统）的设计理念以及实现方向 VMA（virtual memory area） Linux 内核用vm_area_struct结构体描述某一段连续的虚拟内存区域VMA（virtual memory area），每个虚拟内存区域 VMA 都有自己的vm_area_struct 结构体\n内存描述符 mm_struct 指向进程的整个地址空间，vm_area_struct 只是指向了虚拟空间的一段，这块虚拟内存区域VMA的地址范围为 [vm_start, vm_end) ，左开右闭 vm_area_struct 是由双向链表链接起来的，它们是按照虚拟地址降序排序的，每个这样的结构都对应描述一个地址空间范围\n为了快速根据地址找到对应的 VMA，内核对其建立了红黑树索引，红黑树的每个叶子结点就是一个VMA区域，引入红黑树的好处是可以提高查找VMA的效率（即便VMA的数量翻倍，VMA的查找次数也只增加一次） 之所以这样分隔是因为每个虚拟区间可能来源不同，有的可能来自可执行映像，有的可能来自共享库，而有的可能是动态内存分配的内存区，所以对于每个由 vm_area_struct 结构所描述的区间的处理操作和它前后范围的处理操作不同，因此 linux 把虚拟内存分割管理，并利用了虚拟内存处理例程 vm_ops 来抽象对不同来源虚拟内存的处理方法\n不同的虚拟区间其处理操作可能不同，linux 在这里利用了面向对象的思想，即把一个虚拟区间看成是一个对象，用 vm_area_struct 描述这个对象的属性，其中的 vm_operation 结构描述了在这个对象上的操作 虚拟内存空间管理概括图 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # https://elixir.bootlin.com/linux/v2.6.0/source/include/linux/mm.h#L51 struct vm_area_struct { struct mm_struct * vm_mm;\t/* The address space we belong to. */ unsigned long vm_start;\t/* Our start address within vm_mm. */ unsigned long vm_end;\t/* The first byte after our end address within vm_mm. */ /* linked list of VM areas per task, sorted by address */ struct vm_area_struct *vm_next; pgprot_t vm_page_prot;\t/* Access permissions of this VMA. */ unsigned long vm_flags;\t/* Flags, listed below. */ struct rb_node vm_rb; /* * For areas with an address space and backing store, * one of the address_space-\u003ei_mmap{,shared} lists, * for shm areas, the list of attaches, otherwise unused. */ struct list_head shared; /* Function pointers to deal with this struct. */ struct vm_operations_struct * vm_ops; /* Information about our backing store: */ unsigned long vm_pgoff;\t/* Offset (within vm_file) in PAGE_SIZE units, *not* PAGE_CACHE_SIZE */ struct file * vm_file;\t/* File we map to (can be NULL). */ void * vm_private_data;\t/* was vm_pte (shared mem) */ }; struct address_space 看 linux 内核很容易被 struct address_space 这个结构迷惑，它是代表某个地址空间吗？实际上不是的，它是用于管理文件 struct inode 映射到内存的页面 struct page 的，其实就是每个 file 都有这么一个结构，将文件系统中这个 file 对应的数据与这个 file 对应的内存绑定到一起 与之对应，address_space_operations 就是用来操作该文件映射到内存的页面，比如把内存中的修改写回文件、从文件中读入数据到页面缓冲等 一个具体的文件在打开后，内核会在内存中为之建立一个 struct inode 结构（该 inode 结构也会在对应的 file 结构体中引用），其中的 i_mapping 域指向一个 address_space 结构 一个文件就对应一个 address_space 结构，一个 address_space 与一个偏移量能够确定一个 page cache 或 swap cache 中的一个页面，当要寻址某个数据时，很容易根据给定的文件及数据在文件内的偏移量而找到相应的页面 struct file 和 struct inode 结构体中都有一个 struct address_space 指针，实际上，file -\u003e f_mapping 是从对应 inode -\u003e i_mapping 而来, inode -\u003e i_mapping -\u003e a_ops 是由对应的文件系统类型在生成这个 inode 时赋予的 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # https://elixir.bootlin.com/linux/v2.6.0/source/include/linux/fs.h#L319 struct address_space { struct inode\t*host;\t/* owner: inode, block_device */ struct radix_tree_root\tpage_tree;\t/* radix tree of all pages */ spinlock_t\tpage_lock;\t/* and spinlock protecting it */ struct list_head\tclean_pages;\t/* list of clean pages */ struct list_head\tdirty_pages;\t/* list of dirty pages */ struct list_head\tlocked_pages;\t/* list of locked pages */ struct list_head\tio_pages;\t/* being prepared for I/O */ unsigned long\tnrpages;\t/* number of total pages */ struct address_space_operations *a_ops;\t/* methods */ struct list_head\ti_mmap;\t/* list of private mappings */ struct list_head\ti_mmap_shared;\t/* list of shared mappings */ struct semaphore\ti_shared_sem;\t/* protect both above lists */ atomic_t\ttruncate_count;\t/* Cover race condition with truncate */ unsigned long\tdirtied_when;\t/* jiffies of first page dirtying */ unsigned long\tflags;\t/* error bits/gfp mask */ struct backing_dev_info *backing_dev_info; /* device readahead, etc */ spinlock_t\tprivate_lock;\t/* for use by the address_space */ struct list_head\tprivate_list;\t/* ditto */ struct address_space\t*assoc_mapping;\t/* ditto */ }; 文件系统、文件类型、page 的划分 为了方便理解，在此将文件系统划分为内存文件系统（虚拟文件系统）与硬盘文件系统（物理文件系统），在书的这一章节里面，也将广义上的文件分为 virtual file 和 physics file 在书的这一章节里面，将内存页面 page 划分为 anonymous pages （没有物理文件支持的内存页面）与 pages backed by a file（由物理文件映射到内存的某些 pages） page cache 与 swap cache page cache 是与文件映射对应的，而 swap cache 是与匿名页对应的 如果一个内存页面不是文件映射，则在换入换出的时候加入到 swap cache ，如果是文件映射，则不需要交换缓冲 这两个的相同点就是它们都是 address_space ，都有相对应的文件操作：一个被访问的文件的物理页面都驻留在 page cache 或 swap cache 中，一个页面的所有信息由 struct page 来描述 page cache 作用：当文件被读取时，操作系统会把文件内容加载到内存的 page cache 中。如果同一个文件被再次访问，操作系统会直接从 page cache 中读取，而不是再次从磁盘读取，从而减少磁盘访问次数，提高性能 swap cache 作用：swap cache 缓存的是已经交换到磁盘的数据，它是为了提高交换操作的效率，优化虚拟内存的交换操作，如果内存中再次需要使用这些页面，操作系统会从 swap cache 里面寻找，然后再从 swap 里面查找，同时对于那些刚刚从物理内存里面换出来的 page 以及从 swap 空间读取的 page 也会放在 swap cache 里面 一般情况下用户进程调用 mmap() 时，只是在进程空间内新增了一块相应大小的缓冲区，并设置了相应的访问标识，但并没有建立进程空间到物理页面的映射 因此，第一次访问该空间时，会引发一个缺页异常。对于共享内存映射情况，缺页异常处理程序首先在 swap cache 中寻找目标页（符合 address_space 以及偏移量的物理页），如果找到，则直接返回地址；如果没有找到，则判断该页是否在交换区 (swap area)，如果在，则执行一个换入操作；如果上述两种情况都不满足，处理程序将分配新的物理页面，并把它插入到 page cache 中，程最终将更新进程页表 对于映射普通文件情况（非共享映射），缺页异常处理程序首先会在 page cache 中根据 address_space 以及数据偏移量寻找相应的页面，如果没有找到，则说明文件数据还没有读入内存，处理程序会从磁盘读入相应的页面，并返回相应地址，同时，进程页表也会更新 硬盘（物理）文件系统 tmpfs 和 shm 共性与区别 tmpfs 和 shm 都是基于内存的文件系统，它们将文件存储在内存中，而不是磁盘上，尽管它们有一些相似之处，但它们的用途、配置方法以及操作上也有一些关键的区别\ntmpfs 用于存储临时文件，通常挂载到 /tmp 等目录，并提供标准的文件操作接口 shm 主要用于进程间通信，提供共享内存的功能，进程通过 shmget() 和 mmap() 创建和访问共享内存区域 共同点 基于内存：tmpfs 和 shm 都是内存文件系统，文件的数据存储在内存中，而不是磁盘上，因此读写速度非常快\n核心实现： tmpfs 和 shm share core functionality ，比如说对虚拟文件的描述都是使用 shmem_inode_info （ shmem_inode_info 可以看作 inode 的继承，在内存文件系统里面如果要创建一个文件，要先向系统申请一个 inode ，然后才是将这个 inode 传给 shmem_inode_info ，有点类似 C++ 里面的当一个子类要实例化的时候需要先实例化父类）\n1 2 3 4 5 6 7 8 9 10 11 struct shmem_inode_info { spinlock_t\tlock; unsigned long\tnext_index; swp_entry_t\ti_direct[SHMEM_NR_DIRECT]; /* for the first blocks */ struct page\t*i_indirect; /* indirect blocks */ unsigned long\talloced; /* data pages allocated to file */ unsigned long\tswapped; /* subtotal assigned to swap */ unsigned long\tflags; struct list_head\tlist; struct inode\tvfs_inode; }; 至于为什么 tmpfs 采用了与 shm 一样的核心实现，我的看法是，两个都是内存文件系统，并且 tmpfs 可以看作只有一个参与者“共享”的共享内存\n内存管理：它们都使用了 Linux 内存管理机制，文件存储在虚拟内存中，可以像普通文件一样进行操作，同时支持内存映射（mmap()）等内存操作\n支持匿名共享内存：tmpfs 和 shm 都支持共享内存，多个进程可以将文件映射到自己的虚拟内存空间，实现进程间的内存共享\n文件系统类型：它们都是 Linux 内核中的文件系统类型，可以通过 mount 命令挂载到指定路径上\n区别 特性 tmpfs shm 用途 临时文件存储系统，用于存储不需要持久化的临时文件。 主要用于进程间通信（IPC），存储共享内存区域。 挂载点 通常挂载在 /tmp，用于存储临时文件。 通常挂载在 /dev/shm，用于共享内存。 配置方式 需要显式挂载（mount -t tmpfs）。 使用 shmget() 和 mmap() 创建共享内存区域。 文件系统类型 tmpfs 文件系统类型。 shm 是 tmpfs 的一个变体，专门用于共享内存。 文件命名 文件有具体的路径名，文件名可以由用户定义。 文件名由内核管理，通常是匿名的或者以 SYSVNN 命名。 清除方式 文件在系统关闭或卸载时丢失，类似于 RAM 磁盘。 共享内存区域在使用 shmctl() 删除时丢失。 支持进程间通信 不直接支持进程间通信，只是一个临时文件系统。 直接支持进程间的内存共享和通信。 tmpfs 用法： 假设你有一个需要临时存储的文件，并希望它在系统重启时被清除。你可以使用 tmpfs 来挂载一个内存文件系统，并在其中创建文件\n1 2 3 4 5 6 7 8 # 挂载 tmpfs 到 /mnt/tmp mount -t tmpfs tmpfs /mnt/tmp # 创建一个临时文件 echo \"Temporary file content\" \u003e /mnt/tmp/tempfile.txt # 查看文件 cat /mnt/tmp/tempfile.txt 在 tmpfs 里面文件会存储在内存中，重启后文件会消失\ntmpfs 是一个传统的文件系统，文件通过标准的文件操作（如 open()、read()、write() 等）进行访问\nshm 用法： 假设你需要在两个进程之间共享数据，可以使用 shm 来创建共享内存段\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // 创建共享内存，一个大小为 1024 字节的共享内存段 int shmid = shmget(IPC_PRIVATE, 1024, IPC_CREAT | 0666); // 将共享内存附加到进程地址空间 char *shm_ptr = (char *)shmat(shmid, NULL, 0); // 向共享内存写入数据 strcpy(shm_ptr, \"Shared memory data\"); // 在另一个进程中读取共享内存 printf(\"Shared memory content: %s\\n\", shm_ptr); // 删除共享内存 shmctl(shmid, IPC_RMID, NULL); 两个进程可以通过共享内存共享数据\n","wordCount":"879","inLanguage":"zh","image":"https://hcy-asleep.github.io/","datePublished":"2024-11-30T19:35:39Z","dateModified":"2024-11-30T19:35:39Z","author":{"@type":"Person","name":"HCY"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://hcy-asleep.github.io/Linux-%E8%99%9A%E6%8B%9F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%BB%A5%E5%8F%8A%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98%E5%8E%9F%E7%90%86/"},"publisher":{"@type":"Organization","name":"Memos","logo":{"@type":"ImageObject","url":"https://raw.githubusercontent.com/HCY-ASLEEP/picture-bed/main/picture-bed/hcy_site_favicon.png"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://hcy-asleep.github.io/ accesskey=h title="主页 (Alt + H)">主页</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://hcy-asleep.github.io/archives/ title=归档><span>归档</span></a></li><li><a href=https://hcy-asleep.github.io/tags/ title=标签><span>标签</span></a></li><li><a href=https://hcy-asleep.github.io/categories/ title=目录><span>目录</span></a></li><li><a href=https://hcy-asleep.github.io/friends/ title=友链><span>友链</span></a></li><li><a href=https://hcy-asleep.github.io/todo/ title=TODO><span>TODO</span></a></li><li><a href=https://hcy-asleep.github.io/about/ title=关于><span>关于</span></a></li><li><a href=https://hcy-asleep.github.io/search/ title="🔍 (Alt + /)" accesskey=/><span>🔍</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://hcy-asleep.github.io/>主页</a>&nbsp;»&nbsp;<a href=https://hcy-asleep.github.io/post/>Posts</a></div><h1 class="post-title entry-hint-parent">Linux 虚拟文件系统（虚拟内存文件系统）以及共享内存原理</h1><div class=post-meta><span title='2024-11-30 19:35:39 +0000 UTC'>十一月 30, 2024</span>&nbsp;·&nbsp;5 分钟&nbsp;·&nbsp;879 字&nbsp;·&nbsp;HCY&nbsp;|&nbsp;<a href=https://github.com/HCY-ASLEEP rel="noopener noreferrer" target=_blank> Follow me</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#linux-哲学>Linux 哲学</a></li><li><a href=#vmavirtual-memory-area>VMA（virtual memory area）</a></li><li><a href=#struct-address_space><code>struct address_space</code></a></li><li><a href=#文件系统文件类型page-的划分>文件系统、文件类型、<code>page</code> 的划分</a></li><li><a href=#page-cache-与-swap-cache><code>page cache</code> 与 <code>swap cache</code></a></li><li><a href=#硬盘物理文件系统>硬盘（物理）文件系统</a></li><li><a href=#tmpfs-和-shm-共性与区别><code>tmpfs</code> 和 <code>shm</code> 共性与区别</a><ul><li><a href=#共同点>共同点</a></li><li><a href=#区别>区别</a></li><li><a href=#tmpfs-用法><code>tmpfs</code> 用法：</a></li><li><a href=#shm-用法><code>shm</code> 用法：</a></li></ul></li></ul></li></ul></nav></div></details></div><div class=post-content><p>最近看了<a href=https://pdos.csail.mit.edu/~sbw/links/gorman_book.pdf>《Understanding the Linux® Virtual Memory Manager》</a>里面的第十二章 <strong>SHARED MEMORY VIRTUAL FILESYSTEM</strong> ，对文件系统以及内存文件管理有了更加深入的了解，下面是看了这一章节之后对其中一些概念的理解以及拓展，要是想了解这一章，<a href=https://raw.githubusercontent.com/HCY-ASLEEP/picture-bed/main/picture-bed/gordan-book-linux-vmm12-chap.pdf>建议读原文</a>，配合这篇博客辅助理解</p><h2 id=linux-哲学>Linux 哲学<a hidden class=anchor aria-hidden=true href=#linux-哲学>#</a></h2><ul><li>在 Linux 里面，一切皆文件，所有的东西都可以看作一个文件，而凡是文件，都应该支持<code>POSIX</code>文件操作（比如 <code>read()</code> ，<code>write()</code> ，<code>open()</code>）</li><li>每一块内存对象，都可以被看作一个文件，一旦赋予这块内存对象相对应的文件描述，就可以使用像使用普通文件那样子操作内存对象</li><li>而这也正是 <code>VFS</code>（虚拟文件系统 <code>virtual file system</code> ，包括内存文件系统以及共享内存管理系统）的设计理念以及实现方向</li></ul><h2 id=vmavirtual-memory-area>VMA（virtual memory area）<a hidden class=anchor aria-hidden=true href=#vmavirtual-memory-area>#</a></h2><ul><li><p>Linux 内核用<code>vm_area_struct</code>结构体描述某一段连续的虚拟内存区域VMA（virtual memory area），每个虚拟内存区域 <code>VMA</code> 都有自己的<code>vm_area_struct</code> 结构体</p></li><li><p>内存描述符 <code>mm_struct</code> 指向进程的整个地址空间，<code>vm_area_struct</code> 只是指向了虚拟空间的一段，这块虚拟内存区域VMA的地址范围为 <code>[vm_start, vm_end)</code> ，左开右闭
<img src=https://raw.githubusercontent.com/HCY-ASLEEP/picture-bed/main/picture-bed/v2-426557bb4eb1e7044bd649483942c2ad_r.jpg width=55%></p></li><li><p><code>vm_area_struct</code> 是由双向链表链接起来的，它们是按照虚拟地址降序排序的，每个这样的结构都对应描述一个地址空间范围</p></li><li><p>为了快速根据地址找到对应的 <code>VMA</code>，内核对其建立了红黑树索引，红黑树的每个叶子结点就是一个VMA区域，引入红黑树的好处是可以提高查找VMA的效率（即便VMA的数量翻倍，VMA的查找次数也只增加一次）
<input type=checkbox id=zoomCheck-b4749 hidden>
<label for=zoomCheck-b4749><img class=zoomCheck loading=lazy decoding=async src=https://raw.githubusercontent.com/HCY-ASLEEP/picture-bed/main/picture-bed/v2-82dec5bde95009f934d3978c15412626_1440w.jpg alt></label></p></li><li><p>之所以这样分隔是因为每个虚拟区间可能来源不同，有的可能来自可执行映像，有的可能来自共享库，而有的可能是动态内存分配的内存区，所以对于每个由 <code>vm_area_struct</code> 结构所描述的区间的处理操作和它前后范围的处理操作不同，因此 linux 把虚拟内存分割管理，并利用了虚拟内存处理例程 <code>vm_ops</code> 来抽象对不同来源虚拟内存的处理方法</p></li><li><p>不同的虚拟区间其处理操作可能不同，<code>linux</code> 在这里利用了面向对象的思想，即把一个虚拟区间看成是一个对象，用 <code>vm_area_struct</code> 描述这个对象的属性，其中的 <code>vm_operation</code> 结构描述了在这个对象上的操作
<input type=checkbox id=zoomCheck-53ba1 hidden>
<label for=zoomCheck-53ba1><img class=zoomCheck loading=lazy decoding=async src=https://raw.githubusercontent.com/HCY-ASLEEP/picture-bed/main/picture-bed/v2-ce0aa70282615def188d51d5ee745e14_1440w.jpg alt></label></p></li><li><p>虚拟内存空间管理概括图
<input type=checkbox id=zoomCheck-a1d4d hidden>
<label for=zoomCheck-a1d4d><img class=zoomCheck loading=lazy decoding=async src=https://raw.githubusercontent.com/HCY-ASLEEP/picture-bed/main/picture-bed/v2-410e447557fe925077f0580463875666_r.jpg alt></label></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=cp># https:</span><span class=c1>//elixir.bootlin.com/linux/v2.6.0/source/include/linux/mm.h#L51
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>struct</span> <span class=nc>vm_area_struct</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=k>struct</span> <span class=nc>mm_struct</span> <span class=o>*</span> <span class=n>vm_mm</span><span class=p>;</span>	<span class=cm>/* The address space we belong to. */</span>
</span></span><span class=line><span class=cl>	<span class=kt>unsigned</span> <span class=kt>long</span> <span class=n>vm_start</span><span class=p>;</span>		<span class=cm>/* Our start address within vm_mm. */</span>
</span></span><span class=line><span class=cl>	<span class=kt>unsigned</span> <span class=kt>long</span> <span class=n>vm_end</span><span class=p>;</span>		<span class=cm>/* The first byte after our end address
</span></span></span><span class=line><span class=cl><span class=cm>					   within vm_mm. */</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=cm>/* linked list of VM areas per task, sorted by address */</span>
</span></span><span class=line><span class=cl>	<span class=k>struct</span> <span class=nc>vm_area_struct</span> <span class=o>*</span><span class=n>vm_next</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=n>pgprot_t</span> <span class=n>vm_page_prot</span><span class=p>;</span>		<span class=cm>/* Access permissions of this VMA. */</span>
</span></span><span class=line><span class=cl>	<span class=kt>unsigned</span> <span class=kt>long</span> <span class=n>vm_flags</span><span class=p>;</span>		<span class=cm>/* Flags, listed below. */</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=k>struct</span> <span class=nc>rb_node</span> <span class=n>vm_rb</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=cm>/*
</span></span></span><span class=line><span class=cl><span class=cm>	 * For areas with an address space and backing store,
</span></span></span><span class=line><span class=cl><span class=cm>	 * one of the address_space-&gt;i_mmap{,shared} lists,
</span></span></span><span class=line><span class=cl><span class=cm>	 * for shm areas, the list of attaches, otherwise unused.
</span></span></span><span class=line><span class=cl><span class=cm>	 */</span>
</span></span><span class=line><span class=cl>	<span class=k>struct</span> <span class=nc>list_head</span> <span class=n>shared</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=cm>/* Function pointers to deal with this struct. */</span>
</span></span><span class=line><span class=cl>	<span class=k>struct</span> <span class=nc>vm_operations_struct</span> <span class=o>*</span> <span class=n>vm_ops</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=cm>/* Information about our backing store: */</span>
</span></span><span class=line><span class=cl>	<span class=kt>unsigned</span> <span class=kt>long</span> <span class=n>vm_pgoff</span><span class=p>;</span>		<span class=cm>/* Offset (within vm_file) in PAGE_SIZE
</span></span></span><span class=line><span class=cl><span class=cm>					   units, *not* PAGE_CACHE_SIZE */</span>
</span></span><span class=line><span class=cl>	<span class=k>struct</span> <span class=nc>file</span> <span class=o>*</span> <span class=n>vm_file</span><span class=p>;</span>		<span class=cm>/* File we map to (can be NULL). */</span>
</span></span><span class=line><span class=cl>	<span class=kt>void</span> <span class=o>*</span> <span class=n>vm_private_data</span><span class=p>;</span>		<span class=cm>/* was vm_pte (shared mem) */</span>
</span></span><span class=line><span class=cl><span class=p>};</span>
</span></span></code></pre></td></tr></table></div></div></li></ul><h2 id=struct-address_space><code>struct address_space</code><a hidden class=anchor aria-hidden=true href=#struct-address_space>#</a></h2><ul><li>看 <code>linux</code> 内核很容易被 <code>struct address_space</code> 这个结构迷惑，它是代表某个地址空间吗？实际上不是的，它是用于管理文件 <code>struct inode</code> 映射到内存的页面 <code>struct page</code> 的，其实就是每个 <code>file</code> 都有这么一个结构，将文件系统中这个 <code>file</code> 对应的数据与这个 <code>file</code> 对应的内存绑定到一起</li><li>与之对应，<code>address_space_operations</code> 就是用来操作该文件映射到内存的页面，比如把内存中的修改写回文件、从文件中读入数据到页面缓冲等</li><li>一个具体的文件在打开后，内核会在内存中为之建立一个 <code>struct inode</code> 结构（该 <code>inode</code> 结构也会在对应的 <code>file</code> 结构体中引用），其中的 <code>i_mapping</code> 域指向一个 <code>address_space</code> 结构</li><li>一个文件就对应一个 <code>address_space</code> 结构，一个 <code>address_space</code> 与一个偏移量能够确定一个 <code>page cache</code> 或 <code>swap cache</code> 中的一个页面，当要寻址某个数据时，很容易根据给定的文件及数据在文件内的偏移量而找到相应的页面</li><li><code>struct file</code> 和 <code>struct inode</code> 结构体中都有一个 <code>struct address_space</code> 指针，实际上，<code>file -> f_mapping</code> 是从对应 <code>inode -> i_mapping</code> 而来, <code>inode -> i_mapping -> a_ops</code> 是由对应的文件系统类型在生成这个 <code>inode</code> 时赋予的<div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=cp># https:</span><span class=c1>//elixir.bootlin.com/linux/v2.6.0/source/include/linux/fs.h#L319
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>struct</span> <span class=nc>address_space</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=k>struct</span> <span class=nc>inode</span>		<span class=o>*</span><span class=n>host</span><span class=p>;</span>		<span class=cm>/* owner: inode, block_device */</span>
</span></span><span class=line><span class=cl>	<span class=k>struct</span> <span class=nc>radix_tree_root</span>	<span class=n>page_tree</span><span class=p>;</span>	<span class=cm>/* radix tree of all pages */</span>
</span></span><span class=line><span class=cl>	<span class=n>spinlock_t</span>		<span class=n>page_lock</span><span class=p>;</span>	<span class=cm>/* and spinlock protecting it */</span>
</span></span><span class=line><span class=cl>	<span class=k>struct</span> <span class=nc>list_head</span>	<span class=n>clean_pages</span><span class=p>;</span>	<span class=cm>/* list of clean pages */</span>
</span></span><span class=line><span class=cl>	<span class=k>struct</span> <span class=nc>list_head</span>	<span class=n>dirty_pages</span><span class=p>;</span>	<span class=cm>/* list of dirty pages */</span>
</span></span><span class=line><span class=cl>	<span class=k>struct</span> <span class=nc>list_head</span>	<span class=n>locked_pages</span><span class=p>;</span>	<span class=cm>/* list of locked pages */</span>
</span></span><span class=line><span class=cl>	<span class=k>struct</span> <span class=nc>list_head</span>	<span class=n>io_pages</span><span class=p>;</span>	<span class=cm>/* being prepared for I/O */</span>
</span></span><span class=line><span class=cl>	<span class=kt>unsigned</span> <span class=kt>long</span>		<span class=n>nrpages</span><span class=p>;</span>	<span class=cm>/* number of total pages */</span>
</span></span><span class=line><span class=cl>	<span class=k>struct</span> <span class=nc>address_space_operations</span> <span class=o>*</span><span class=n>a_ops</span><span class=p>;</span>	<span class=cm>/* methods */</span>
</span></span><span class=line><span class=cl>	<span class=k>struct</span> <span class=nc>list_head</span>	<span class=n>i_mmap</span><span class=p>;</span>		<span class=cm>/* list of private mappings */</span>
</span></span><span class=line><span class=cl>	<span class=k>struct</span> <span class=nc>list_head</span>	<span class=n>i_mmap_shared</span><span class=p>;</span>	<span class=cm>/* list of shared mappings */</span>
</span></span><span class=line><span class=cl>	<span class=k>struct</span> <span class=nc>semaphore</span>	<span class=n>i_shared_sem</span><span class=p>;</span>	<span class=cm>/* protect both above lists */</span>
</span></span><span class=line><span class=cl>	<span class=n>atomic_t</span>		<span class=n>truncate_count</span><span class=p>;</span>	<span class=cm>/* Cover race condition with truncate */</span>
</span></span><span class=line><span class=cl>	<span class=kt>unsigned</span> <span class=kt>long</span>		<span class=n>dirtied_when</span><span class=p>;</span>	<span class=cm>/* jiffies of first page dirtying */</span>
</span></span><span class=line><span class=cl>	<span class=kt>unsigned</span> <span class=kt>long</span>		<span class=n>flags</span><span class=p>;</span>		<span class=cm>/* error bits/gfp mask */</span>
</span></span><span class=line><span class=cl>	<span class=k>struct</span> <span class=nc>backing_dev_info</span> <span class=o>*</span><span class=n>backing_dev_info</span><span class=p>;</span> <span class=cm>/* device readahead, etc */</span>
</span></span><span class=line><span class=cl>	<span class=n>spinlock_t</span>		<span class=n>private_lock</span><span class=p>;</span>	<span class=cm>/* for use by the address_space */</span>
</span></span><span class=line><span class=cl>	<span class=k>struct</span> <span class=nc>list_head</span>	<span class=n>private_list</span><span class=p>;</span>	<span class=cm>/* ditto */</span>
</span></span><span class=line><span class=cl>	<span class=k>struct</span> <span class=nc>address_space</span>	<span class=o>*</span><span class=n>assoc_mapping</span><span class=p>;</span>	<span class=cm>/* ditto */</span>
</span></span><span class=line><span class=cl><span class=p>};</span>
</span></span></code></pre></td></tr></table></div></div></li></ul><h2 id=文件系统文件类型page-的划分>文件系统、文件类型、<code>page</code> 的划分<a hidden class=anchor aria-hidden=true href=#文件系统文件类型page-的划分>#</a></h2><ul><li>为了方便理解，在此将文件系统划分为内存文件系统（虚拟文件系统）与硬盘文件系统（物理文件系统），在书的这一章节里面，也将广义上的文件分为 <code>virtual file</code> 和 <code>physics file</code></li><li>在书的这一章节里面，将内存页面 <code>page</code> 划分为 <code>anonymous pages</code> （没有物理文件支持的内存页面）与 <code>pages backed by a file</code>（由物理文件映射到内存的某些 <code>pages</code>）</li></ul><h2 id=page-cache-与-swap-cache><code>page cache</code> 与 <code>swap cache</code><a hidden class=anchor aria-hidden=true href=#page-cache-与-swap-cache>#</a></h2><ul><li><code>page cache</code> 是与文件映射对应的，而 <code>swap cache</code> 是与匿名页对应的</li><li>如果一个内存页面不是文件映射，则在换入换出的时候加入到 <code>swap cache</code> ，如果是文件映射，则不需要交换缓冲</li><li>这两个的相同点就是它们都是 <code>address_space</code> ，都有相对应的文件操作：一个被访问的文件的物理页面都驻留在 <code>page cache</code> 或 <code>swap cache</code> 中，一个页面的所有信息由 <code>struct page</code> 来描述</li><li><code>page cache</code> 作用：当文件被读取时，操作系统会把文件内容加载到内存的 <code>page cache</code> 中。如果同一个文件被再次访问，操作系统会直接从 <code>page cache</code> 中读取，而不是再次从磁盘读取，从而减少磁盘访问次数，提高性能</li><li><code>swap cache</code> 作用：<code>swap cache</code> 缓存的是已经交换到磁盘的数据，它是为了提高交换操作的效率，优化虚拟内存的交换操作，如果内存中再次需要使用这些页面，操作系统会从 <code>swap cache</code> 里面寻找，然后再从 <code>swap</code> 里面查找，同时对于那些刚刚从物理内存里面换出来的 <code>page</code> 以及从 <code>swap</code> 空间读取的 <code>page</code> 也会放在 <code>swap cache</code> 里面</li><li>一般情况下用户进程调用 <code>mmap()</code> 时，只是在进程空间内新增了一块相应大小的缓冲区，并设置了相应的访问标识，但并没有建立进程空间到物理页面的映射</li><li>因此，第一次访问该空间时，会引发一个缺页异常。对于共享内存映射情况，缺页异常处理程序首先在 <code>swap cache</code> 中寻找目标页（符合 <code>address_space</code> 以及偏移量的物理页），如果找到，则直接返回地址；如果没有找到，则判断该页是否在交换区 (<code>swap area</code>)，如果在，则执行一个换入操作；如果上述两种情况都不满足，处理程序将分配新的物理页面，并把它插入到 <code>page cache</code> 中，程最终将更新进程页表</li><li>对于映射普通文件情况（非共享映射），缺页异常处理程序首先会在 <code>page cache</code> 中根据 <code>address_space</code> 以及数据偏移量寻找相应的页面，如果没有找到，则说明文件数据还没有读入内存，处理程序会从磁盘读入相应的页面，并返回相应地址，同时，进程页表也会更新</li></ul><h2 id=硬盘物理文件系统>硬盘（物理）文件系统<a hidden class=anchor aria-hidden=true href=#硬盘物理文件系统>#</a></h2><h2 id=tmpfs-和-shm-共性与区别><code>tmpfs</code> 和 <code>shm</code> 共性与区别<a hidden class=anchor aria-hidden=true href=#tmpfs-和-shm-共性与区别>#</a></h2><p><code>tmpfs</code> 和 <code>shm</code> 都是基于内存的文件系统，它们将文件存储在内存中，而不是磁盘上，尽管它们有一些相似之处，但它们的用途、配置方法以及操作上也有一些关键的区别</p><ul><li><strong><code>tmpfs</code></strong> 用于存储临时文件，通常挂载到 <code>/tmp</code> 等目录，并提供标准的文件操作接口</li><li><strong><code>shm</code></strong> 主要用于进程间通信，提供共享内存的功能，进程通过 <code>shmget()</code> 和 <code>mmap()</code> 创建和访问共享内存区域</li></ul><h3 id=共同点>共同点<a hidden class=anchor aria-hidden=true href=#共同点>#</a></h3><ul><li><p><strong>基于内存</strong>：<code>tmpfs</code> 和 <code>shm</code> 都是内存文件系统，文件的数据存储在内存中，而不是磁盘上，因此读写速度非常快</p></li><li><p><strong>核心实现</strong>： <code>tmpfs</code> 和 <code>shm</code> <code>share core functionality</code> ，比如说对虚拟文件的描述都是使用 <code>shmem_inode_info</code> （ <code>shmem_inode_info</code> 可以看作 <code>inode</code> 的继承，在内存文件系统里面如果要创建一个文件，要先向系统申请一个 <code>inode</code> ，然后才是将这个 <code>inode</code> 传给 <code>shmem_inode_info</code> ，有点类似 C++ 里面的当一个子类要实例化的时候需要先实例化父类）</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=k>struct</span> <span class=nc>shmem_inode_info</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=n>spinlock_t</span>		<span class=n>lock</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=kt>unsigned</span> <span class=kt>long</span>		<span class=n>next_index</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=n>swp_entry_t</span>		<span class=n>i_direct</span><span class=p>[</span><span class=n>SHMEM_NR_DIRECT</span><span class=p>];</span> <span class=cm>/* for the first blocks */</span>
</span></span><span class=line><span class=cl>	<span class=k>struct</span> <span class=nc>page</span>	       <span class=o>*</span><span class=n>i_indirect</span><span class=p>;</span> <span class=cm>/* indirect blocks */</span>
</span></span><span class=line><span class=cl>	<span class=kt>unsigned</span> <span class=kt>long</span>		<span class=n>alloced</span><span class=p>;</span>    <span class=cm>/* data pages allocated to file */</span>
</span></span><span class=line><span class=cl>	<span class=kt>unsigned</span> <span class=kt>long</span>		<span class=n>swapped</span><span class=p>;</span>    <span class=cm>/* subtotal assigned to swap */</span>
</span></span><span class=line><span class=cl>	<span class=kt>unsigned</span> <span class=kt>long</span>		<span class=n>flags</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=k>struct</span> <span class=nc>list_head</span>	<span class=n>list</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=k>struct</span> <span class=nc>inode</span>		<span class=n>vfs_inode</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>};</span>
</span></span></code></pre></td></tr></table></div></div><p>至于为什么 <code>tmpfs</code> 采用了与 <code>shm</code> 一样的核心实现，我的看法是，两个都是内存文件系统，并且 <code>tmpfs</code> 可以看作只有一个参与者“共享”的共享内存</p></li><li><p><strong>内存管理</strong>：它们都使用了 Linux 内存管理机制，文件存储在虚拟内存中，可以像普通文件一样进行操作，同时支持内存映射（<code>mmap()</code>）等内存操作</p></li><li><p><strong>支持匿名共享内存</strong>：<code>tmpfs</code> 和 <code>shm</code> 都支持共享内存，多个进程可以将文件映射到自己的虚拟内存空间，实现进程间的内存共享</p></li><li><p><strong>文件系统类型</strong>：它们都是 Linux 内核中的文件系统类型，可以通过 <code>mount</code> 命令挂载到指定路径上</p></li></ul><h3 id=区别>区别<a hidden class=anchor aria-hidden=true href=#区别>#</a></h3><table><thead><tr><th>特性</th><th><strong>tmpfs</strong></th><th><strong>shm</strong></th></tr></thead><tbody><tr><td><strong>用途</strong></td><td>临时文件存储系统，用于存储不需要持久化的临时文件。</td><td>主要用于进程间通信（IPC），存储共享内存区域。</td></tr><tr><td><strong>挂载点</strong></td><td>通常挂载在 <code>/tmp</code>，用于存储临时文件。</td><td>通常挂载在 <code>/dev/shm</code>，用于共享内存。</td></tr><tr><td><strong>配置方式</strong></td><td>需要显式挂载（<code>mount -t tmpfs</code>）。</td><td>使用 <code>shmget()</code> 和 <code>mmap()</code> 创建共享内存区域。</td></tr><tr><td><strong>文件系统类型</strong></td><td><code>tmpfs</code> 文件系统类型。</td><td><code>shm</code> 是 <code>tmpfs</code> 的一个变体，专门用于共享内存。</td></tr><tr><td><strong>文件命名</strong></td><td>文件有具体的路径名，文件名可以由用户定义。</td><td>文件名由内核管理，通常是匿名的或者以 <code>SYSVNN</code> 命名。</td></tr><tr><td><strong>清除方式</strong></td><td>文件在系统关闭或卸载时丢失，类似于 RAM 磁盘。</td><td>共享内存区域在使用 <code>shmctl()</code> 删除时丢失。</td></tr><tr><td><strong>支持进程间通信</strong></td><td>不直接支持进程间通信，只是一个临时文件系统。</td><td>直接支持进程间的内存共享和通信。</td></tr></tbody></table><h3 id=tmpfs-用法><code>tmpfs</code> 用法：<a hidden class=anchor aria-hidden=true href=#tmpfs-用法>#</a></h3><p>假设你有一个需要临时存储的文件，并希望它在系统重启时被清除。你可以使用 <code>tmpfs</code> 来挂载一个内存文件系统，并在其中创建文件</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 挂载 tmpfs 到 /mnt/tmp</span>
</span></span><span class=line><span class=cl>mount -t tmpfs tmpfs /mnt/tmp
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 创建一个临时文件</span>
</span></span><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;Temporary file content&#34;</span> &gt; /mnt/tmp/tempfile.txt
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 查看文件</span>
</span></span><span class=line><span class=cl>cat /mnt/tmp/tempfile.txt
</span></span></code></pre></td></tr></table></div></div><p>在 <code>tmpfs</code> 里面文件会存储在内存中，重启后文件会消失</p><p><code>tmpfs</code> 是一个传统的文件系统，文件通过标准的文件操作（如 <code>open()</code>、<code>read()</code>、<code>write()</code> 等）进行访问</p><h3 id=shm-用法><code>shm</code> 用法：<a hidden class=anchor aria-hidden=true href=#shm-用法>#</a></h3><p>假设你需要在两个进程之间共享数据，可以使用 <code>shm</code> 来创建共享内存段</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=c1>// 创建共享内存，一个大小为 1024 字节的共享内存段
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>int</span> <span class=n>shmid</span> <span class=o>=</span> <span class=nf>shmget</span><span class=p>(</span><span class=n>IPC_PRIVATE</span><span class=p>,</span> <span class=mi>1024</span><span class=p>,</span> <span class=n>IPC_CREAT</span> <span class=o>|</span> <span class=mo>0666</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// 将共享内存附加到进程地址空间
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>char</span> <span class=o>*</span><span class=n>shm_ptr</span> <span class=o>=</span> <span class=p>(</span><span class=kt>char</span> <span class=o>*</span><span class=p>)</span><span class=nf>shmat</span><span class=p>(</span><span class=n>shmid</span><span class=p>,</span> <span class=nb>NULL</span><span class=p>,</span> <span class=mi>0</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// 向共享内存写入数据
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=nf>strcpy</span><span class=p>(</span><span class=n>shm_ptr</span><span class=p>,</span> <span class=s>&#34;Shared memory data&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// 在另一个进程中读取共享内存
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Shared memory content: %s</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>shm_ptr</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// 删除共享内存
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=nf>shmctl</span><span class=p>(</span><span class=n>shmid</span><span class=p>,</span> <span class=n>IPC_RMID</span><span class=p>,</span> <span class=nb>NULL</span><span class=p>);</span>
</span></span></code></pre></td></tr></table></div></div><p>两个进程可以通过共享内存共享数据</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://hcy-asleep.github.io/Linux%E7%A3%81%E7%9B%98%E5%8D%A0%E7%94%A8%E5%88%86%E6%9E%90/><span class=title>下一页 »</span><br><span>Linux磁盘占用分析</span></a></nav></footer></br></br><script src=https://giscus.app/client.js data-repo=HCY-ASLEEP/HCY-ASLEEP.github.io data-repo-id=R_kgDOISFjNg data-category=Announcements data-category-id=DIC_kwDOISFjNs4CUJyb data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=light data-lang=zh-CN crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://hcy-asleep.github.io/>Memos</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>